{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import numpy as np\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "import lightning.pytorch as L\n",
    "import rootutils\n",
    "from lightning.pytorch import Callback, LightningDataModule, LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import Logger, WandbLogger\n",
    "\n",
    "rootutils.setup_root(os.path.abspath(''), indicator=['.git', 'pyproject.toml'], pythonpath=True)\n",
    "\n",
    "from src.utils import (\n",
    "    instantiate_callbacks,\n",
    "    instantiate_loggers,\n",
    "    log_hyperparameters,\n",
    "    set_precision,\n",
    ")\n",
    "from src.utils.utils import close_loggers\n",
    "\n",
    "from src.utils import rich_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base='1.3', config_path='../configs'):\n",
    "    cfg = compose(config_name='train.yaml', return_hydra_config=True, overrides=['experiment=train_seat_cls',\n",
    "                                                                                 'trainer.max_epochs=40',\n",
    "                                                                                 'model=cnn_effnet_v2_s_full',\n",
    "                                                                                 'model.optimizer.lr=0.000067',\n",
    "                                                                                 'data.batch_size=64',\n",
    "                                                                                 'logger=many_loggers',\n",
    "                                                                                 'callbacks.model_summary=None',\n",
    "                                                                                 'paths.log_dir=../logs',\n",
    "                                                                                 'paths.output_dir=../logs'])\n",
    "    HydraConfig.instance().set_config(cfg)\n",
    "    \n",
    "rich_utils.print_config_tree(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.get('seed'):\n",
    "    L.seed_everything(cfg.seed, workers=True)\n",
    "if cfg.precision.get('float32_matmul'):\n",
    "    set_precision(cfg.precision.float32_matmul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from src.data.components.preprocessing.preproc_pipeline_manager import PreprocessingPipeline\n",
    "from src.data.components.utils import clear_directory\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "class ClassificationDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = 'data/',\n",
    "        preprocessing_pipeline: PreprocessingPipeline = None,\n",
    "        overwrite_data: bool = False,\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        train_transforms: Compose = None,\n",
    "        val_test_transforms: Compose = None,\n",
    "        save_predict_images: bool = False,\n",
    "        num_classes: int = 2,\n",
    "        discard_idx: list[int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a `DirDataModule`.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str, optional): The data directory. Defaults to 'data/'.\n",
    "            preprocessing_pipeline (PreprocessingPipeline, optional): Custom preprocessing pipeline. Defaults to None.\n",
    "            batch_size (int, optional): Batch size. Defaults to 64.\n",
    "            num_workers (int, optional): Number of workers. Defaults to 0.\n",
    "            pin_memory (bool, optional): Whether to pin memory. Defaults to False.\n",
    "            train_transforms (Compose, optional): Train split transformations. Defaults to None.\n",
    "            val_test_transforms (Compose, optional): Validation and test split transformations. Defaults to None.\n",
    "            save_predict_images (bool, optional): Save images in predict mode? Defaults to False.\n",
    "            num_classes (int, optional): Number of classes in the dataset.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.dataset = None\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "        self.preprocessed_data: dict[Path] = {}\n",
    "\n",
    "        self.discard_idx = discard_idx\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Get the number of classes.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of classes (2).\n",
    "        \"\"\"\n",
    "        return self.hparams.num_classes\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Data preparation hook.\"\"\"\n",
    "\n",
    "        data_path = Path(self.hparams.data_dir)\n",
    "        base_path = data_path.parent\n",
    "        last_subdir = data_path.name\n",
    "        output_path = base_path / f'{last_subdir}_processed'\n",
    "\n",
    "        initial_data = {'initial_data': self.hparams.data_dir}\n",
    "        if output_path.exists():\n",
    "            if self.hparams.overwrite_data:\n",
    "                clear_directory(output_path)\n",
    "                output_path.rmdir()\n",
    "                self.preprocessed_data = self.hparams.preprocessing_pipeline.run(\n",
    "                    initial_data\n",
    "                )\n",
    "            else:\n",
    "                self.preprocessed_data = (\n",
    "                    self.hparams.preprocessing_pipeline.get_processed_data_path(\n",
    "                        initial_data\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            self.preprocessed_data = self.hparams.preprocessing_pipeline.run(\n",
    "                initial_data\n",
    "            )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Datamodule setup step.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str], optional): The stage to setup. Either `\"fit\"`,\n",
    "            `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to None.\n",
    "        \"\"\"\n",
    "        if stage in {'fit', 'validate'}:\n",
    "            # Create separate datasets\n",
    "            train_dataset = ImageFolder(\n",
    "                root=self.preprocessed_data['train'],\n",
    "                transform=self.hparams.train_transforms,\n",
    "            )\n",
    "            val_dataset = ImageFolder(\n",
    "                root=self.preprocessed_data['val'],\n",
    "                transform=self.hparams.train_transforms,\n",
    "            )\n",
    "\n",
    "            # Merge datasets\n",
    "            self.dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "            # Indices that are not discarded\n",
    "            if self.discard_idx:\n",
    "                indices = list(range(len(self.dataset)))\n",
    "                indices = [i for i in indices if i not in self.discard_idx]\n",
    "                self.dataset = Subset(self.dataset, indices)\n",
    "\n",
    "        # Split dataset into train and validation\n",
    "        train_size = int(0.7 * len(self.dataset))  # 70% for training\n",
    "        val_size = len(self.dataset) - train_size  # 30% for validation\n",
    "        self.data_train, self.data_val = random_split(self.dataset, [train_size, val_size])\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.data_test = ImageFolder(\n",
    "                root=self.preprocessed_data['test'],\n",
    "                transform=self.hparams.val_test_transforms,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the train dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The train dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_train, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the validation dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The validation dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_val, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the test dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The test dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_test, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the predict dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The predict dataloader.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Lightning hook for cleaning up after `trainer.fit()`, `trainer.validate()`,\n",
    "        `trainer.test()`, and `trainer.predict()`.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str], optional): The stage being torn down. Either `\"fit\"`,\n",
    "            `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to None.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def state_dict(self) -> dict[Any, Any]:\n",
    "        \"\"\"Called when saving a checkpoint. Implement to generate and save the datamodule state.\n",
    "\n",
    "        Returns:\n",
    "            Dict[Any, Any]: A dictionary containing the datamodule state that you want to save.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n",
    "        \"\"\"Called when loading a checkpoint. Implement to reload datamodule state given datamodule\n",
    "        `state_dict()`.\n",
    "\n",
    "        Args:\n",
    "            state_dict (Dict[str, Any]): The datamodule state returned by `self.state_dict()`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _default_dataloader(\n",
    "        self, dataset: Dataset, shuffle: bool = False\n",
    "    ) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return a dataloader.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to use.\n",
    "            shuffle (bool, optional): Flag for shuffling data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: Pytorch dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            persistent_workers=True,\n",
    "            shuffle=shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discarded_indices = np.load(\"discarded_indices.npy\")\n",
    "discarded_indices = discarded_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: LightningModule = hydra.utils.instantiate(cfg.model)\n",
    "\n",
    "loggers: list[Logger] = instantiate_loggers(cfg.get('logger'))\n",
    "has_wandb = any(isinstance(logger, WandbLogger) for logger in loggers)\n",
    "callbacks: list[Callback] = instantiate_callbacks(cfg.get('callbacks'), has_wandb=has_wandb)\n",
    "\n",
    "trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=loggers)\n",
    "\n",
    "preprocessing_pipeline = hydra.utils.instantiate(cfg.data.preprocessing_pipeline)\n",
    "train_transforms = hydra.utils.instantiate(cfg.data.train_transforms)\n",
    "val_test_transforms = hydra.utils.instantiate(cfg.data.val_test_transforms)\n",
    "datamodule = ClassificationDataModule(data_dir=cfg.data.data_dir,\n",
    "                                        preprocessing_pipeline=preprocessing_pipeline,\n",
    "                                        overwrite_data=cfg.data.overwrite_data,\n",
    "                                        batch_size=cfg.data.batch_size,\n",
    "                                        num_workers=cfg.data.num_workers,\n",
    "                                        pin_memory=cfg.data.pin_memory,\n",
    "                                        train_transforms=train_transforms,\n",
    "                                        val_test_transforms=val_test_transforms,\n",
    "                                        save_predict_images=cfg.data.save_predict_images,\n",
    "                                        num_classes=cfg.data.num_classes,\n",
    "                                        discard_idx=discarded_indices)\n",
    "object_dict = {\n",
    "    'cfg': cfg,\n",
    "    'datamodule': datamodule,\n",
    "    'model': model,\n",
    "    'callbacks': callbacks,\n",
    "    'logger': loggers,\n",
    "    'trainer': trainer,\n",
    "}\n",
    "\n",
    "if loggers:\n",
    "    log_hyperparameters(object_dict)\n",
    "\n",
    "if cfg.get('train'):\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "train_metrics = trainer.callback_metrics\n",
    "\n",
    "close_loggers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepVisionXplain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
