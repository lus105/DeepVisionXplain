{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rootutils\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "rootutils.setup_root(os.path.abspath(''), indicator=['.git', 'pyproject.toml'], pythonpath=True)\n",
    "\n",
    "from src.utils import rich_utils\n",
    "from src.models.components.cnn_cam_multihead import CNNCAMMultihead\n",
    "from src.models.components.nn_utils import weight_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base='1.3', config_path='../configs'):\n",
    "    cfg = compose(config_name='train.yaml', return_hydra_config=True, overrides=['experiment=train_seat_cls',\n",
    "                                                                                 'logger=many_loggers',\n",
    "                                                                                 'callbacks.model_summary=None',\n",
    "                                                                                 'paths.log_dir=../logs',\n",
    "                                                                                 'paths.output_dir=../logs'])\n",
    "    HydraConfig.instance().set_config(cfg)\n",
    "    \n",
    "rich_utils.print_config_tree(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom datamodule class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "from lightning.pytorch import LightningDataModule\n",
    "\n",
    "from src.data.components.preprocessing.preproc_pipeline_manager import PreprocessingPipeline\n",
    "from src.data.components.utils import clear_directory\n",
    "\n",
    "\n",
    "class ClassificationDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = 'data/',\n",
    "        preprocessing_pipeline: PreprocessingPipeline = None,\n",
    "        overwrite_data: bool = False,\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        train_transforms: Compose = None,\n",
    "        val_test_transforms: Compose = None,\n",
    "        save_predict_images: bool = False,\n",
    "        num_classes: int = 2,\n",
    "        k_folds: int = 5,\n",
    "        current_fold: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a `DirDataModule`.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str, optional): The data directory. Defaults to 'data/'.\n",
    "            preprocessing_pipeline (PreprocessingPipeline, optional): Custom preprocessing pipeline. Defaults to None.\n",
    "            batch_size (int, optional): Batch size. Defaults to 64.\n",
    "            num_workers (int, optional): Number of workers. Defaults to 0.\n",
    "            pin_memory (bool, optional): Whether to pin memory. Defaults to False.\n",
    "            train_transforms (Compose, optional): Train split transformations. Defaults to None.\n",
    "            val_test_transforms (Compose, optional): Validation and test split transformations. Defaults to None.\n",
    "            save_predict_images (bool, optional): Save images in predict mode? Defaults to False.\n",
    "            num_classes (int, optional): Number of classes in the dataset.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.dataset = None\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "        self.preprocessed_data: dict[Path] = {}\n",
    "\n",
    "        self.k_folds = k_folds\n",
    "        self.current_fold = current_fold\n",
    "        self.kfold = None\n",
    "        self.indices = None\n",
    "        self.val_ixd = None\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Get the number of classes.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of classes (2).\n",
    "        \"\"\"\n",
    "        return self.hparams.num_classes\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Data preparation hook.\"\"\"\n",
    "\n",
    "        data_path = Path(self.hparams.data_dir)\n",
    "        base_path = data_path.parent\n",
    "        last_subdir = data_path.name\n",
    "        output_path = base_path / f'{last_subdir}_processed'\n",
    "\n",
    "        initial_data = {'initial_data': self.hparams.data_dir}\n",
    "        if output_path.exists():\n",
    "            if self.hparams.overwrite_data:\n",
    "                clear_directory(output_path)\n",
    "                output_path.rmdir()\n",
    "                self.preprocessed_data = self.hparams.preprocessing_pipeline.run(\n",
    "                    initial_data\n",
    "                )\n",
    "            else:\n",
    "                self.preprocessed_data = (\n",
    "                    self.hparams.preprocessing_pipeline.get_processed_data_path(\n",
    "                        initial_data\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            self.preprocessed_data = self.hparams.preprocessing_pipeline.run(\n",
    "                initial_data\n",
    "            )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Datamodule setup step.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str], optional): The stage to setup. Either `\"fit\"`,\n",
    "            `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to None.\n",
    "        \"\"\"\n",
    "        if stage in {'fit', 'validate'}:\n",
    "            # Create separate datasets\n",
    "            train_dataset = ImageFolder(\n",
    "                root=self.preprocessed_data['train'],\n",
    "                transform=self.hparams.train_transforms,\n",
    "            )\n",
    "            val_dataset = ImageFolder(\n",
    "                root=self.preprocessed_data['val'],\n",
    "                transform=self.hparams.train_transforms,\n",
    "            )\n",
    "\n",
    "            # Merge datasets\n",
    "            self.dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "            self.image_paths = [path for dataset in [train_dataset, val_dataset] for path, _ in dataset.samples]\n",
    "\n",
    "            # Create KFold splitter\n",
    "            self.kfold = KFold(n_splits=self.k_folds, shuffle=True, random_state=42)\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "            folds = list(self.kfold.split(self.indices))\n",
    "\n",
    "            # Get train and validation indices for the current fold\n",
    "            train_idx, val_idx = folds[self.current_fold]\n",
    "            self.val_idx = val_idx\n",
    "            self.data_train = Subset(self.dataset, train_idx)\n",
    "            self.data_val = Subset(self.dataset, val_idx)\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.data_test = ImageFolder(\n",
    "                root=self.preprocessed_data['test'],\n",
    "                transform=self.hparams.val_test_transforms,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the train dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The train dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_train, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the validation dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The validation dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_val, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the test dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The test dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_test, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the predict dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The predict dataloader.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Lightning hook for cleaning up after `trainer.fit()`, `trainer.validate()`,\n",
    "        `trainer.test()`, and `trainer.predict()`.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str], optional): The stage being torn down. Either `\"fit\"`,\n",
    "            `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to None.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def state_dict(self) -> dict[Any, Any]:\n",
    "        \"\"\"Called when saving a checkpoint. Implement to generate and save the datamodule state.\n",
    "\n",
    "        Returns:\n",
    "            Dict[Any, Any]: A dictionary containing the datamodule state that you want to save.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n",
    "        \"\"\"Called when loading a checkpoint. Implement to reload datamodule state given datamodule\n",
    "        `state_dict()`.\n",
    "\n",
    "        Args:\n",
    "            state_dict (Dict[str, Any]): The datamodule state returned by `self.state_dict()`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _default_dataloader(\n",
    "        self, dataset: Dataset, shuffle: bool = False\n",
    "    ) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return a dataloader.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to use.\n",
    "            shuffle (bool, optional): Flag for shuffling data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: Pytorch dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            persistent_workers=True,\n",
    "            shuffle=shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNCAMMultihead(\n",
    "    backbone='torchvision.models/efficientnet_v2_s',\n",
    "    return_node='features.7',\n",
    "    multi_head=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datamodule setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "preprocessing_pipeline = hydra.utils.instantiate(cfg.data.preprocessing_pipeline)\n",
    "train_transforms = hydra.utils.instantiate(cfg.data.train_transforms)\n",
    "val_test_transforms = hydra.utils.instantiate(cfg.data.val_test_transforms)\n",
    "datamodule = ClassificationDataModule(data_dir=cfg.data.data_dir,\n",
    "                                    preprocessing_pipeline=preprocessing_pipeline,\n",
    "                                    overwrite_data=cfg.data.overwrite_data,\n",
    "                                    batch_size=cfg.data.batch_size,\n",
    "                                    num_workers=cfg.data.num_workers,\n",
    "                                    pin_memory=cfg.data.pin_memory,\n",
    "                                    train_transforms=train_transforms,\n",
    "                                    val_test_transforms=val_test_transforms,\n",
    "                                    save_predict_images=cfg.data.save_predict_images,\n",
    "                                    num_classes=cfg.data.num_classes,\n",
    "                                    k_folds=num_folds,\n",
    "                                    current_fold=0)\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup(stage='validate')\n",
    "np.save(\"image_paths.npy\", np.array(datamodule.image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_ckpt_map = {\n",
    "        0: 'fold_0.ckpt',\n",
    "        1: 'fold_1.ckpt',\n",
    "        2: 'fold_2.ckpt',\n",
    "        3: 'fold_3.ckpt',\n",
    "        4: 'fold_4.ckpt',\n",
    "    }\n",
    "\n",
    "num_samples = len(datamodule.dataset)\n",
    "num_features = model.feature_extractor.n_features\n",
    "num_classes = 2 # binary classification\n",
    "\n",
    "features = np.zeros((num_samples, num_features))\n",
    "pred_probs = np.zeros((num_samples, num_classes))\n",
    "labels_out = np.zeros((num_samples, num_classes-1))\n",
    "\n",
    "for fold_id, ckpt_name in fold_ckpt_map.items():\n",
    "    # loading weights for fold\n",
    "    weights = weight_load(\n",
    "        ckpt_path='../trained_models/fold_training/' + ckpt_name,\n",
    "        weights_only=False,\n",
    "    )\n",
    "    model.load_state_dict(weights)\n",
    "    model.eval()\n",
    "\n",
    "    # initialize datamodule with selected fold\n",
    "    datamodule = ClassificationDataModule(data_dir=cfg.data.data_dir,\n",
    "                                          preprocessing_pipeline=preprocessing_pipeline,\n",
    "                                          overwrite_data=cfg.data.overwrite_data,\n",
    "                                          batch_size=cfg.data.batch_size,\n",
    "                                          num_workers=cfg.data.num_workers,\n",
    "                                          pin_memory=cfg.data.pin_memory,\n",
    "                                          train_transforms=train_transforms,\n",
    "                                          val_test_transforms=val_test_transforms,\n",
    "                                          save_predict_images=cfg.data.save_predict_images,\n",
    "                                          num_classes=cfg.data.num_classes,\n",
    "                                          k_folds=num_folds,\n",
    "                                          current_fold=fold_id)\n",
    "\n",
    "    # get samples from datamodule validation set\n",
    "    datamodule.prepare_data()\n",
    "    datamodule.setup(stage='validate')\n",
    "    val_loader = datamodule.val_dataloader()\n",
    "    val_idx = datamodule.val_idx\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred_probs_fold = []\n",
    "        features_fold = []\n",
    "        labels_fold = []\n",
    "\n",
    "        for batch in val_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            output = model(images)\n",
    "            feature_map = model.feature_extractor(images)\n",
    "            embeddings = torch.flatten(torch.nn.functional.adaptive_avg_pool2d(feature_map, (1, 1)), 1)\n",
    "\n",
    "            # Convert (N,1) sigmoid output to (N,2) probability format\n",
    "            output_np = output.cpu().numpy()\n",
    "            output_prob = np.hstack([1 - output_np, output_np])  # Shape (N,2)\n",
    "\n",
    "            pred_probs_fold.append(output_prob)\n",
    "            features_fold.append(embeddings.cpu().numpy())\n",
    "            labels_fold.append(labels.cpu().numpy().reshape(-1, 1))\n",
    "\n",
    "        features[val_idx] = np.concatenate(features_fold, axis=0)\n",
    "        pred_probs[val_idx] = np.concatenate(pred_probs_fold, axis=0)\n",
    "        labels_out[val_idx] = np.concatenate(labels_fold, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"features.npy\", features)\n",
    "np.save(\"pred_probs.npy\", pred_probs)\n",
    "np.save(\"labels.npy\", labels_out)\n",
    "print(\"Saved features.npy, pred_probs.npy and labels.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepVisionXplain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
