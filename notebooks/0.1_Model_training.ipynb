{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "import lightning.pytorch as L\n",
    "import rootutils\n",
    "from lightning.pytorch import Callback, LightningDataModule, LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import Logger, WandbLogger\n",
    "\n",
    "rootutils.setup_root(os.path.abspath(''), indicator=['.git', 'pyproject.toml'], pythonpath=True)\n",
    "\n",
    "from src.utils import (\n",
    "    instantiate_callbacks,\n",
    "    instantiate_loggers,\n",
    "    log_hyperparameters,\n",
    "    set_precision,\n",
    ")\n",
    "\n",
    "from src.utils import rich_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base='1.3', config_path='../configs'):\n",
    "    cfg = compose(config_name='train.yaml', return_hydra_config=True, overrides=['experiment=train_seat_cls',\n",
    "                                                                                 'logger=many_loggers',\n",
    "                                                                                 'callbacks.model_summary=None',\n",
    "                                                                                 'paths.log_dir=../logs',\n",
    "                                                                                 'paths.output_dir=../logs'])\n",
    "    HydraConfig.instance().set_config(cfg)\n",
    "    \n",
    "rich_utils.print_config_tree(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.get('seed'):\n",
    "    L.seed_everything(cfg.seed, workers=True)\n",
    "if cfg.precision.get('float32_matmul'):\n",
    "    set_precision(cfg.precision.float32_matmul)\n",
    "loggers: list[Logger] = instantiate_loggers(cfg.get('logger'))\n",
    "has_wandb = any(isinstance(logger, WandbLogger) for logger in loggers)\n",
    "callbacks: list[Callback] = instantiate_callbacks(cfg.get('callbacks'), has_wandb=has_wandb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Optional\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from src.data.components.preprocessing.preproc_pipeline_manager import PreprocessingPipeline\n",
    "from src.data.components.utils import clear_directory\n",
    "\n",
    "\n",
    "class ClassificationDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = 'data/',\n",
    "        preprocessing_pipeline: PreprocessingPipeline = None,\n",
    "        overwrite_data: bool = False,\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "        train_transforms: Compose = None,\n",
    "        val_test_transforms: Compose = None,\n",
    "        save_predict_images: bool = False,\n",
    "        num_classes: int = 2,\n",
    "        k_folds: int = 5,\n",
    "        current_fold: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a `DirDataModule`.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str, optional): The data directory. Defaults to 'data/'.\n",
    "            preprocessing_pipeline (PreprocessingPipeline, optional): Custom preprocessing pipeline. Defaults to None.\n",
    "            batch_size (int, optional): Batch size. Defaults to 64.\n",
    "            num_workers (int, optional): Number of workers. Defaults to 0.\n",
    "            pin_memory (bool, optional): Whether to pin memory. Defaults to False.\n",
    "            train_transforms (Compose, optional): Train split transformations. Defaults to None.\n",
    "            val_test_transforms (Compose, optional): Validation and test split transformations. Defaults to None.\n",
    "            save_predict_images (bool, optional): Save images in predict mode? Defaults to False.\n",
    "            num_classes (int, optional): Number of classes in the dataset.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        self.dataset = None\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "        self.preprocessed_data: dict[Path] = {}\n",
    "\n",
    "        self.k_folds = k_folds\n",
    "        self.current_fold = current_fold\n",
    "        self.kfold = None\n",
    "        self.indices = None\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Get the number of classes.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of classes (2).\n",
    "        \"\"\"\n",
    "        return self.hparams.num_classes\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Data preparation hook.\"\"\"\n",
    "\n",
    "        data_path = Path(self.hparams.data_dir)\n",
    "        base_path = data_path.parent\n",
    "        last_subdir = data_path.name\n",
    "        output_path = base_path / f'{last_subdir}_processed'\n",
    "\n",
    "        initial_data = {'initial_data': self.hparams.data_dir}\n",
    "        if output_path.exists():\n",
    "            if self.hparams.overwrite_data:\n",
    "                clear_directory(output_path)\n",
    "                output_path.rmdir()\n",
    "                self.preprocessed_data = self.hparams.preprocessing_pipeline.run(\n",
    "                    initial_data\n",
    "                )\n",
    "            else:\n",
    "                self.preprocessed_data = (\n",
    "                    self.hparams.preprocessing_pipeline.get_processed_data_path(\n",
    "                        initial_data\n",
    "                    )\n",
    "                )\n",
    "        else:\n",
    "            self.preprocessed_data = self.hparams.preprocessing_pipeline.run(\n",
    "                initial_data\n",
    "            )\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Datamodule setup step.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str], optional): The stage to setup. Either `\"fit\"`,\n",
    "            `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to None.\n",
    "        \"\"\"\n",
    "        if stage in {'fit', 'validate'}:\n",
    "            # Create separate datasets\n",
    "            train_dataset = ImageFolder(\n",
    "                root=self.preprocessed_data['train'],\n",
    "                transform=self.hparams.train_transforms,\n",
    "            )\n",
    "            val_dataset = ImageFolder(\n",
    "                root=self.preprocessed_data['val'],\n",
    "                transform=self.hparams.train_transforms,\n",
    "            )\n",
    "\n",
    "            # Merge datasets\n",
    "            self.dataset = ConcatDataset([train_dataset, val_dataset])\n",
    "\n",
    "            # Create KFold splitter\n",
    "            self.kfold = KFold(n_splits=self.k_folds, shuffle=True, random_state=42)\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "            folds = list(self.kfold.split(self.indices))\n",
    "\n",
    "            # Get train and validation indices for the current fold\n",
    "            train_idx, val_idx = folds[self.current_fold]\n",
    "            self.data_train = Subset(self.dataset, train_idx)\n",
    "            self.data_val = Subset(self.dataset, val_idx)\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.data_test = ImageFolder(\n",
    "                root=self.preprocessed_data['test'],\n",
    "                transform=self.hparams.val_test_transforms,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the train dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The train dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_train, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the validation dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The validation dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_val, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the test dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The test dataloader.\n",
    "        \"\"\"\n",
    "        return self._default_dataloader(self.data_test, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the predict dataloader.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: The predict dataloader.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Lightning hook for cleaning up after `trainer.fit()`, `trainer.validate()`,\n",
    "        `trainer.test()`, and `trainer.predict()`.\n",
    "\n",
    "        Args:\n",
    "            stage (Optional[str], optional): The stage being torn down. Either `\"fit\"`,\n",
    "            `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to None.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def state_dict(self) -> dict[Any, Any]:\n",
    "        \"\"\"Called when saving a checkpoint. Implement to generate and save the datamodule state.\n",
    "\n",
    "        Returns:\n",
    "            Dict[Any, Any]: A dictionary containing the datamodule state that you want to save.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n",
    "        \"\"\"Called when loading a checkpoint. Implement to reload datamodule state given datamodule\n",
    "        `state_dict()`.\n",
    "\n",
    "        Args:\n",
    "            state_dict (Dict[str, Any]): The datamodule state returned by `self.state_dict()`.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _default_dataloader(\n",
    "        self, dataset: Dataset, shuffle: bool = False\n",
    "    ) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return a dataloader.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to use.\n",
    "            shuffle (bool, optional): Flag for shuffling data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader[Any]: Pytorch dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            persistent_workers=True,\n",
    "            shuffle=shuffle,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "preprocessing_pipeline = hydra.utils.instantiate(cfg.data.preprocessing_pipeline)\n",
    "train_transforms = hydra.utils.instantiate(cfg.data.train_transforms)\n",
    "val_test_transforms = hydra.utils.instantiate(cfg.data.val_test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(num_folds):\n",
    "    model: LightningModule = hydra.utils.instantiate(cfg.model)\n",
    "    trainer: Trainer = hydra.utils.instantiate(cfg.trainer, callbacks=callbacks, logger=loggers)\n",
    "    datamodule = ClassificationDataModule(data_dir=cfg.data.data_dir,\n",
    "                                          preprocessing_pipeline=preprocessing_pipeline,\n",
    "                                          overwrite_data=cfg.data.overwrite_data,\n",
    "                                          batch_size=cfg.data.batch_size,\n",
    "                                          num_workers=cfg.data.num_workers,\n",
    "                                          pin_memory=cfg.data.pin_memory,\n",
    "                                          train_transforms=train_transforms,\n",
    "                                          val_test_transforms=val_test_transforms,\n",
    "                                          save_predict_images=cfg.data.save_predict_images,\n",
    "                                          num_classes=cfg.data.num_classes,\n",
    "                                          k_folds=num_folds,\n",
    "                                          current_fold=fold)\n",
    "    object_dict = {\n",
    "        'cfg': cfg,\n",
    "        'datamodule': datamodule,\n",
    "        'model': model,\n",
    "        'callbacks': callbacks,\n",
    "        'logger': loggers,\n",
    "        'trainer': trainer,\n",
    "    }\n",
    "\n",
    "    if loggers:\n",
    "        log_hyperparameters(object_dict)\n",
    "\n",
    "    if cfg.get('train'):\n",
    "        trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "    train_metrics = trainer.callback_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepVisionXplain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
