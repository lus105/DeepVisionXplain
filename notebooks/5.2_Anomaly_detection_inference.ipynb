{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import rootutils\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# adding root to python path\n",
    "rootutils.setup_root(\n",
    "    os.path.abspath(''), indicator=['.git', 'pyproject.toml'], pythonpath=True\n",
    ")\n",
    "\n",
    "from src.models.components.nn_utils import weight_load\n",
    "from src.data.components.utils import list_files\n",
    "from src.models.components.base_model import BaseModel\n",
    "from src.data.components.preprocessing.preproc_strategy_tile import sliding_window_with_coordinates\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image: np.array, model: torch.nn.Module, device: torch.device, transform) -> np.array:\n",
    "    transformed = transform(image=image)\n",
    "    image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(image_tensor)\n",
    "    out = torch.nn.functional.interpolate(out, size=image.shape[:2], mode=\"bilinear\", align_corners=False)\n",
    "    mask = torch.sigmoid(out[0])\n",
    "    mask = (mask > 0.5).float()\n",
    "\n",
    "    mask = mask.detach().cpu().numpy()\n",
    "    mask = (mask[0] * 255).astype('uint8')\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_align_image(image: np.array, mask: np.array) -> np.array:\n",
    "    # Ensure the mask is binary\n",
    "    _, mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)\n",
    "    # Compute the bounding box of the mask\n",
    "    x, y, w, h = cv2.boundingRect(mask)\n",
    "    # Compute the center of the bounding box\n",
    "    bbox_cx, bbox_cy = x + w // 2, y + h // 2  # (x, y) coordinates\n",
    "    # Create an empty black image of the same size\n",
    "    H, W = image.shape[:2]\n",
    "    output = np.zeros_like(image)\n",
    "    # Extract the masked region from the original image\n",
    "    masked_region = cv2.bitwise_and(image, image, mask=mask)\n",
    "    # Compute the new center position (center of output image)\n",
    "    new_cx, new_cy = W // 2, H // 2  # Image center\n",
    "    # Compute translation offsets\n",
    "    dx, dy = int(new_cx - bbox_cx), int(new_cy - bbox_cy)\n",
    "    # Create a translation matrix\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "    # Move the masked region to the new position\n",
    "    moved_masked_region = cv2.warpAffine(masked_region, M, (W, H))\n",
    "    # Move the mask itself to match the new position\n",
    "    moved_mask = cv2.warpAffine(mask, M, (W, H))\n",
    "    # Combine only the valid (non-zero) parts into the output image\n",
    "    output[moved_mask > 0] = moved_masked_region[moved_mask > 0]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_background(image: np.ndarray, background_perc: float) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if an image has a background percentage of black pixels\n",
    "    greater than the specified threshold.\n",
    "\n",
    "    Args:\n",
    "        background_perc (float): The threshold percentage for black pixels.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the percentage of black pixels is greater than the threshold, False otherwise.\n",
    "    \"\"\"\n",
    "    black_pixels = np.all(image == 0, axis=-1)\n",
    "    black_pixel_count = np.sum(black_pixels)\n",
    "    total_pixels = image.shape[0] * image.shape[1]\n",
    "    black_pixel_percentage = black_pixel_count / total_pixels\n",
    "    return black_pixel_percentage > background_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tiles(image: np.array, tile_size: tuple[int, int] = (224, 224), ovelap: int = 0, background_th: float = 0.8) -> list[np.array]:\n",
    "    tiles = []\n",
    "    for tile, coordinates in sliding_window_with_coordinates(image, tile_size=tile_size, overlap=ovelap):\n",
    "        if is_background(tile, background_th):\n",
    "            continue\n",
    "        tiles.append(tile)\n",
    "\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model: torch.nn.Module, image_tiles: list[np.array], device: torch.device, transform) -> np.array:\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for img in image_tiles:\n",
    "            img = transform(img)\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            emb = model(img).flatten(start_dim=1)\n",
    "            embeddings.append(emb)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, 0)\n",
    "    embeddings = normalize(embeddings.cpu().numpy())\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(query_emb: np.ndarray, reference_emb: np.ndarray) -> np.ndarray:\n",
    "    # Calculate pairwise distances using broadcasting\n",
    "    distances = np.linalg.norm(query_emb[:, np.newaxis] - reference_emb, axis=2)\n",
    "    # Find indices of minimum distances along axis 1\n",
    "    nearest_indices = np.argmin(distances, axis=1)\n",
    "    # Extract minimum distances\n",
    "    min_distances = np.min(distances, axis=1)\n",
    "\n",
    "    for i, (idx, dist) in enumerate(zip(nearest_indices, min_distances)):\n",
    "        print(f\"Vector {i} nearest neighbor is at index {idx} with distance: {dist:.4f}\")\n",
    "\n",
    "    # Plot distances\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(len(min_distances)), min_distances, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Query Vector Index')\n",
    "    plt.ylabel('Distance to Nearest Neighbor')\n",
    "    plt.title('Distance of Each Query Vector to its Nearest Neighbor')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return nearest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_model = BaseModel(\n",
    "    model_name = 'segmentation_models_pytorch/Segformer',\n",
    "    encoder_name = 'resnet50',\n",
    "    encoder_weigths = 'imagenet',\n",
    "    num_classes = 1\n",
    "    ).to(device)\n",
    "segmentation_weights = weight_load(\n",
    "    ckpt_path='../trained_models/segformer.ckpt',\n",
    "    weights_only=True,\n",
    ")\n",
    "segmentation_model.load_state_dict(segmentation_weights)\n",
    "segmentation_model.eval()\n",
    "\n",
    "segmentation_transform = A.Compose([\n",
    "    A.Resize(\n",
    "        height = 768,\n",
    "        width = 640\n",
    "        ),\n",
    "    A.ToFloat(max_value=255),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18()\n",
    "embedding_model = torch.nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "\n",
    "embedding_weights = weight_load(\n",
    "    ckpt_path='../trained_models/contrastive_model.ckpt',\n",
    "    weights_only=True,\n",
    "    remove_prefix='backbone.'\n",
    ")\n",
    "embedding_model.load_state_dict(embedding_weights, strict=False)\n",
    "embedding_model.eval()\n",
    "\n",
    "embedding_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_embeddings = np.load(\"good_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(os.environ.get('lear_bad_data_path'))\n",
    "image_paths = list_files(data_path, file_extensions=['.bmp', '.jpg', '.png'])\n",
    "print(f'Found {len(image_paths)} images')\n",
    "\n",
    "output_path = Path('tiles')\n",
    "output_path.mkdir(exist_ok=True)\n",
    "tile_size = (512, 512)\n",
    "overlap = 20\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = cv2.imread(str(image_path), cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    mask = segment_image(image, segmentation_model, device, segmentation_transform)\n",
    "    aligned_image = crop_and_align_image(image, mask)\n",
    "    tiles = extract_tiles(aligned_image, tile_size=tile_size, ovelap=overlap)\n",
    "    #save tiles\n",
    "    for i, tile in enumerate(tiles):\n",
    "        cv2.imwrite(f\"tiles/tile_{i}.png\", cv2.cvtColor(tile, cv2.COLOR_RGB2BGR))\n",
    "    embeddings = generate_embeddings(embedding_model, tiles, device, embedding_transform)\n",
    "    print(image_path.name)\n",
    "    nearest_indices = find_nearest_neighbors(embeddings, good_embeddings)\n",
    "    #break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepVisionXplain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
