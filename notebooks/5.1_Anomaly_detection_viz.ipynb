{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import rootutils\n",
    "import torch\n",
    "import torchvision\n",
    "from sklearn.preprocessing import normalize\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import umap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "\n",
    "# adding root to python path\n",
    "rootutils.setup_root(\n",
    "    os.path.abspath(''), indicator=['.git', 'pyproject.toml'], pythonpath=True\n",
    ")\n",
    "\n",
    "from src.models.components.nn_utils import weight_load\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(os.environ.get('lear_good_data_path'))\n",
    "\n",
    "test_transform = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "data_test = LightlyDataset(\n",
    "    input_dir=data_path,\n",
    "    transform=test_transform\n",
    "    )\n",
    "\n",
    "dataloader_test = DataLoader(\n",
    "    dataset=data_test,\n",
    "    batch_size=10,\n",
    "    num_workers=3,\n",
    "    persistent_workers=True,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18()\n",
    "model = torch.nn.Sequential(*list(resnet.children())[:-1]).to(device)\n",
    "\n",
    "weights = weight_load(\n",
    "    ckpt_path='../trained_models/contrastive_model.ckpt',\n",
    "    weights_only=True,\n",
    "    remove_prefix='backbone.'\n",
    ")\n",
    "\n",
    "model.load_state_dict(weights, strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, dataloader):\n",
    "    \"\"\"Generates representations for all images in the dataloader with\n",
    "    the given model\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        for img, _, fnames in dataloader:\n",
    "            img = img.to(device)\n",
    "            emb = model(img).flatten(start_dim=1)\n",
    "            embeddings.append(emb)\n",
    "            filenames.extend(fnames)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, 0)\n",
    "    embeddings = normalize(embeddings.cpu().numpy())\n",
    "    return embeddings, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, filenames = generate_embeddings(model, dataloader_test)\n",
    "np.save('good_embeddings.npy', embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_as_np_array(filename: str):\n",
    "    \"\"\"Returns an image as an numpy array\"\"\"\n",
    "    img = Image.open(filename)\n",
    "    return np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn_examples(embeddings, filenames, n_neighbors=5, num_examples=30):\n",
    "    \"\"\"Plots multiple rows of random images with their nearest neighbors\"\"\"\n",
    "    # lets look at the nearest neighbors for some samples\n",
    "    # we use the sklearn library\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "\n",
    "    # get 5 random samples\n",
    "    samples_idx = np.random.choice(len(indices), size=num_examples, replace=False)\n",
    "\n",
    "    # loop through our randomly picked samples\n",
    "    for idx in samples_idx:\n",
    "        fig = plt.figure()\n",
    "        # loop through their nearest neighbors\n",
    "        for plot_x_offset, neighbor_idx in enumerate(indices[idx]):\n",
    "            # add the subplot\n",
    "            ax = fig.add_subplot(1, len(indices[idx]), plot_x_offset + 1)\n",
    "            # get the correponding filename for the current index\n",
    "            fname = os.path.join(data_path, filenames[neighbor_idx])\n",
    "            # plot the image\n",
    "            plt.imshow(get_image_as_np_array(fname))\n",
    "            # set the title to the distance of the neighbor\n",
    "            ax.set_title(f\"d={distances[idx][plot_x_offset]:.3f}\")\n",
    "            # let's disable the axis\n",
    "            plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_knn_examples(embeddings, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_features(embeddings, filenames, num_samples=1000):\n",
    "    \"\"\"Plots UMAP visualization of embeddings with colors based on classes and legend outside plot\"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "    sampled_indices = np.random.choice(len(embeddings), size=min(num_samples, len(embeddings)), replace=False)\n",
    "    sampled_embeddings = embeddings[sampled_indices]\n",
    "    sampled_filenames = [filenames[i] for i in sampled_indices]\n",
    "\n",
    "    # Extract classes from filenames (assumes format 'class\\\\name.png')\n",
    "    classes = [fname.split('\\\\')[0] for fname in sampled_filenames]\n",
    "    unique_classes = list(set(classes))\n",
    "    class_to_color = {cls: i for i, cls in enumerate(unique_classes)}\n",
    "    colors = [class_to_color[cls] for cls in classes]\n",
    "\n",
    "    umap_embeddings = reducer.fit_transform(sampled_embeddings)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    scatter = ax.scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], c=colors, cmap='tab10', s=5)\n",
    "\n",
    "    # Create custom legend manually\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markersize=5, markerfacecolor=plt.get_cmap('tab10')(i))\n",
    "        for i, cls in enumerate(unique_classes)\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, labels=unique_classes, title=\"Classes\",\n",
    "              bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.title('UMAP visualization of image embeddings by class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_features(embeddings, filenames, num_samples=len(filenames))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepVisionXplain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
